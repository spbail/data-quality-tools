{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "touched-isaac",
   "metadata": {},
   "source": [
    "# The Wonderful World of Data Quality Tools in Python\n",
    "Sam Bail, Data Umbrella, March 2021"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "gentle-crime",
   "metadata": {},
   "source": [
    "## Imports and data loading\n",
    "We've got some CSV files with 10,000 row samples of NYC taxi ride data from January and February 2019 which I'm loading here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "perceived-implement",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import warnings\n",
    "warnings.simplefilter('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "backed-affairs",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>vendor_id</th>\n",
       "      <th>pickup_datetime</th>\n",
       "      <th>dropoff_datetime</th>\n",
       "      <th>passenger_count</th>\n",
       "      <th>fare_amount</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2</td>\n",
       "      <td>2019-01-05 06:36:51</td>\n",
       "      <td>2019-01-05 06:50:42</td>\n",
       "      <td>1</td>\n",
       "      <td>14.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>2019-01-23 15:22:13</td>\n",
       "      <td>2019-01-23 15:32:50</td>\n",
       "      <td>1</td>\n",
       "      <td>12.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>2019-01-04 10:54:47</td>\n",
       "      <td>2019-01-04 11:18:31</td>\n",
       "      <td>2</td>\n",
       "      <td>17.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>2019-01-05 12:07:08</td>\n",
       "      <td>2019-01-05 12:14:06</td>\n",
       "      <td>1</td>\n",
       "      <td>6.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2</td>\n",
       "      <td>2019-01-04 18:23:00</td>\n",
       "      <td>2019-01-04 18:25:22</td>\n",
       "      <td>5</td>\n",
       "      <td>3.5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   vendor_id      pickup_datetime     dropoff_datetime  passenger_count  \\\n",
       "0          2  2019-01-05 06:36:51  2019-01-05 06:50:42                1   \n",
       "1          1  2019-01-23 15:22:13  2019-01-23 15:32:50                1   \n",
       "2          2  2019-01-04 10:54:47  2019-01-04 11:18:31                2   \n",
       "3          1  2019-01-05 12:07:08  2019-01-05 12:14:06                1   \n",
       "4          2  2019-01-04 18:23:00  2019-01-04 18:25:22                5   \n",
       "\n",
       "   fare_amount  \n",
       "0         14.0  \n",
       "1         12.5  \n",
       "2         17.0  \n",
       "3          6.0  \n",
       "4          3.5  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "csv1 = 'data/yellow_tripdata_sample_2019-01.csv'\n",
    "df1 = pd.read_csv(csv1)\n",
    "df1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sacred-ethiopia",
   "metadata": {},
   "outputs": [],
   "source": [
    "csv2 = 'data/yellow_tripdata_sample_2019-02.csv'\n",
    "df2 = pd.read_csv(csv2)\n",
    "df2.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "external-subject",
   "metadata": {},
   "source": [
    "## Example 1: Pandas \"describe\" for DataFrames\n",
    "A simple \"profiler\" for dataframes. It just gives you some basic statistics on numeric columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "secret-instrumentation",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "emotional-declaration",
   "metadata": {},
   "source": [
    "# Example 2: Pandas Profiling\n",
    "Like a very sophisticated extension of .describe() on Pandas dataframes, \n",
    "creates a more detailed profile report of the data.\n",
    "\n",
    "**Note:** The key difference between the two dataframes is the minimum in the *passenger_count* column:\n",
    "* In the January data (df1), we have passenger counts from 1 through 6. \n",
    "* In the February data (df2), we have counts from 0 through 6, which looks like a bug."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "wrapped-terrace",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple way to profile our dataframe and look at the nicely rendered HTML result\n",
    "\n",
    "from pandas_profiling import ProfileReport\n",
    "ProfileReport(df1, title=\"Pandas Profiling Report for df1\").to_notebook_iframe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "electronic-celtic",
   "metadata": {},
   "outputs": [],
   "source": [
    "ProfileReport(df2, title=\"Pandas Profiling Report for df2\").to_notebook_iframe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "liquid-captain",
   "metadata": {},
   "source": [
    "# Example 3: TDDA (Test-Driven Data Analysis)\n",
    "TDDA allows us to generate \"constraints\" from a reference data asset and verify whether another data asset matches those constraints."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "stretch-steps",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate the constraints based on the January dataframe\n",
    "\n",
    "from tdda.constraints import discover_df, verify_df\n",
    "constraints = discover_df(df1)\n",
    "constraints_path = 'tdda_refs/example_constraints.tdda'\n",
    "with open(constraints_path, 'w') as f:\n",
    "    f.write(constraints.to_json())\n",
    "    \n",
    "# Show the generated constraints\n",
    "print(str(constraints))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "honest-prefix",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify that the January data matches the constraints - this should match of course!\n",
    "\n",
    "v1 = verify_df(df1, constraints_path, type_checking='strict', epsilon=0)\n",
    "print(str(v1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adopted-addition",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify that the February data matches the constraints - this should fail\n",
    "# because we have a different min for passenger_count\n",
    "\n",
    "v2 = verify_df(df2, constraints_path, type_checking='strict', epsilon=0)\n",
    "print(str(v2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "honey-collaboration",
   "metadata": {},
   "source": [
    "## Example 4: Bulwark\n",
    "Data testing framework that lets you add tests on methods that return Pandas dataframes. \n",
    "Has some built-in tests and allows custom methods for tests. List of all built-in tests (\"checks\"): https://bulwark.readthedocs.io/en/stable/bulwark.checks.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "considered-throw",
   "metadata": {},
   "outputs": [],
   "source": [
    "import bulwark.decorators as dc\n",
    "\n",
    "# Option 1: Add checks/assertions as decorators on methods that generate dataframes\n",
    "# This will return the df if all tests pass, and raise errors if any of the tests fail\n",
    "@dc.HasNoNans()\n",
    "@dc.IsShape((10000, 5)) # 10000 rows, 5 columns\n",
    "@dc.HasValsWithinRange(items={\"passenger_count\": (1,6)}) # min and max are inclusive here\n",
    "def load_and_test_csv(csv_file_path):\n",
    "    df = pd.read_csv(csv_file_path)\n",
    "    return df\n",
    "\n",
    "# Test this out with the January data\n",
    "load_and_test_csv(csv1).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "chinese-refund",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now let's test the February data - this should fail the \"has vals within range\" test\n",
    "load_and_test_csv(csv2).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "loving-spray",
   "metadata": {},
   "outputs": [],
   "source": [
    "import bulwark.checks as ck\n",
    "\n",
    "# Option 2: You can also use the built-in tests (\"checks\") directly on a dataframe\n",
    "ck.is_shape(df2, (10000, 5)).head() # 10000 rows, 5 columns\n",
    "ck.has_vals_within_range(df2, items={\"passenger_count\": (1,6)}).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "nutritional-drawing",
   "metadata": {},
   "source": [
    "## Example 5: Voluptous/Opulent Pandas\n",
    "Voluptous is a data validation library that allows you to specify a \"schema\" to validate JSON/YAML. \n",
    "Opulent Pandas is a df-focused “version” of Voluptuous. The syntax to define and validate a schema\n",
    "is very similar. \n",
    "\n",
    "**Note:** Opulent Pandas doesn't look like it's being actively maintained, so I'm\n",
    "only showing Voluptuous here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "public-typing",
   "metadata": {},
   "outputs": [],
   "source": [
    "from voluptuous import All, Range, ALLOW_EXTRA\n",
    "\n",
    "# I had to fiddle a little to pass the right dict-type data structure into the schema\n",
    "# This returns the df if the tests pass, or throws an error if a test fails \n",
    "def validate_df(df):\n",
    "    schema = Schema(\n",
    "        {\n",
    "            'vendor_id': All(int)\n",
    "            'passenger_count': All(int, Range(min=1, max=6))\n",
    "        }, \n",
    "        extra=ALLOW_EXTRA\n",
    "    )\n",
    "    for r in df.to_dict('records'):\n",
    "        schema(r)\n",
    "    return df\n",
    "\n",
    "# Test this out with df1, which should pass\n",
    "validate_df(df1).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acquired-fortune",
   "metadata": {},
   "outputs": [],
   "source": [
    "validate_df(df2).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "certified-microwave",
   "metadata": {},
   "source": [
    "## Example 6: mobydq\n",
    "\n",
    "Data validation web app that allows you to check for \"indicators\" such as completeness, freshness, latency, validity.\n",
    "Only showing a screenshot here because the setup is pretty heavyweight.\n",
    "\n",
    "![mobydq](img/mobydq_screenshot.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "humanitarian-phoenix",
   "metadata": {},
   "source": [
    "## Example 7: dvc (data version control)\n",
    "Command-line tool, showing screenshots.\n",
    "\n",
    "\n",
    "![dvc init](img/dvc_init.png)\n",
    "![dvc add](img/dvc_add.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "early-pilot",
   "metadata": {},
   "source": [
    "You can then make modifications to data files and commit new versions and check out previous versions, \n",
    "all controlled via the .dvc file. \n",
    "dvc also allows you to set remotes to sync data to (instead of managing the actual data via GitHub), e.g. S3."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "russian-terminal",
   "metadata": {},
   "source": [
    "## Example 8: dedupe\n",
    "Uses fuzzy matching to perform de-duplication and entity resolution in data. This isn't super useful with my sample data as the taxi ride records are unlikely to be \"fuzzy\" dupes. We could perhaps records to be duplicates if they match in all fields and have timestamps that are within a certain threshold."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "continuing-extreme",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pickup_datetime : 2019-01-26 22:57:27\n",
      "fare_amount : 21.0\n",
      "\n",
      "pickup_datetime : 2019-01-26 22:57:38\n",
      "fare_amount : 11.5\n",
      "\n",
      "0/10 positive, 0/10 negative\n",
      "Do these records refer to the same thing?\n",
      "(y)es / (n)o / (u)nsure / (f)inished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pickup_datetime : 2019-01-18 10:43:40\n",
      "fare_amount : 10.0\n",
      "\n",
      "pickup_datetime : 2019-01-18 10:44:12\n",
      "fare_amount : 41.5\n",
      "\n",
      "1/10 positive, 0/10 negative\n",
      "Do these records refer to the same thing?\n",
      "(y)es / (n)o / (u)nsure / (f)inished / (p)revious\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pickup_datetime : 2019-01-30 18:50:23\n",
      "fare_amount : 7.0\n",
      "\n",
      "pickup_datetime : 2019-01-30 19:08:31\n",
      "fare_amount : 9.5\n",
      "\n",
      "1/10 positive, 1/10 negative\n",
      "Do these records refer to the same thing?\n",
      "(y)es / (n)o / (u)nsure / (f)inished / (p)revious\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pickup_datetime : 2019-01-22 09:09:42\n",
      "fare_amount : 7.5\n",
      "\n",
      "pickup_datetime : 2019-01-22 09:09:05\n",
      "fare_amount : 6.0\n",
      "\n",
      "1/10 positive, 2/10 negative\n",
      "Do these records refer to the same thing?\n",
      "(y)es / (n)o / (u)nsure / (f)inished / (p)revious\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pickup_datetime : 2019-01-04 06:47:43\n",
      "fare_amount : 11.5\n",
      "\n",
      "pickup_datetime : 2019-01-04 06:48:30\n",
      "fare_amount : 33.0\n",
      "\n",
      "2/10 positive, 2/10 negative\n",
      "Do these records refer to the same thing?\n",
      "(y)es / (n)o / (u)nsure / (f)inished / (p)revious\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Finished labeling\n"
     ]
    }
   ],
   "source": [
    "import dedupe\n",
    "import logging\n",
    "\n",
    "logging.getLogger().setLevel(logging.WARNING)\n",
    "\n",
    "input_file = csv1\n",
    "output_file = 'dedupe_files/csv_example_output.csv'\n",
    "settings_file = 'dedupe_files/csv_example_learned_settings'\n",
    "training_file = 'dedupe_files/csv_example_training.json'\n",
    "\n",
    "data = pd.read_csv(csv1)\n",
    "\n",
    "# Define which fields we want deduper to consider\n",
    "fields = [\n",
    "    {'field': 'pickup_datetime', 'type': 'DateTime'},\n",
    "    {'field': 'fare_amount', 'type': 'Price'},\n",
    "]\n",
    "deduper = dedupe.Dedupe(fields)\n",
    "\n",
    "# Input to prepare_training needs to be a dict of index: dict elements\n",
    "deduper.prepare_training(data.to_dict('index'))\n",
    "\n",
    "# Interactive labeler allows us to confirm/reject whether records are dupes\n",
    "dedupe.console_label(deduper)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "based-representative",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model!\n",
    "deduper.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "whole-button",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save our weights and predicates to disk. If the settings file exists, \n",
    "# we will skip all the training and learning next time we run this file.\n",
    "\n",
    "with open(training_file, 'w') as tf:\n",
    "    deduper.write_training(tf)\n",
    "with open(settings_file, 'wb') as sf:\n",
    "    deduper.write_settings(sf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "another-polymer",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Partition will return sets of records that dedupe believes are all referring to the same entity.\n",
    "\n",
    "# This throws an error because we don't have a cluster in our sample data :)\n",
    "try:\n",
    "    clustered_dupes = deduper.partition(data, threshold=0.5) \n",
    "    print('# duplicate sets:', len(clustered_dupes))\n",
    "except Exception as e:\n",
    "    pass\n",
    "\n",
    "# clustered_dupes will contain a \"cluster ID\" for each record which we can use\n",
    "# to combine or remove duplicate records from the same cluster"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
